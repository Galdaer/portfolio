"""
Medical Mirrors - Local mirrors for Healthcare MCP API sources
Provides unlimited access to PubMed, ClinicalTrials.gov, and FDA databases
"""

import logging
import os
from datetime import datetime

from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()
from collections.abc import AsyncGenerator
from contextlib import asynccontextmanager
from typing import Any

import uvicorn
from billing_codes.api import (
    get_billing_categories,
    get_billing_code_details,
    get_billing_stats,
    search_billing_codes,
)
from clinicaltrials.api import ClinicalTrialsAPI
from drugs.api import DrugAPI
from fastapi import BackgroundTasks, FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from health_info.api import (
    get_exercise_details,
    get_food_details,
    get_health_info_stats,
    get_health_topic_details,
    search_exercises,
    search_foods,
    search_health_topics,
)

# Import new API modules
from icd10.api import (
    get_icd10_categories,
    get_icd10_code_details,
    get_icd10_stats,
    search_icd10_codes,
)
from pubmed.api import PubMedAPI
from pubmed.api_optimized import OptimizedPubMedAPI
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

from config import Config
from database import Base, get_database_url

# Configure logging
logging.basicConfig(
    level=os.getenv("LOG_LEVEL", "INFO"),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# Initialize database
DATABASE_URL = get_database_url()
engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)


@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    """Application lifespan management"""
    logger.info("Starting Medical Mirrors API")

    # Create database tables
    try:
        Base.metadata.create_all(bind=engine)
        logger.info("Database tables created/verified")
    except Exception as e:
        logger.exception(f"Database initialization failed: {e}")
        raise

    yield

    logger.info("Shutting down Medical Mirrors API")


# Initialize FastAPI app
app = FastAPI(
    title="Medical Mirrors API",
    description="Local mirrors for Healthcare MCP API sources - unlimited access to medical data",
    version="1.0.0",
    lifespan=lifespan,
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize API handlers
config = Config()

# Use optimized multi-core parser if enabled
if hasattr(config, "ENABLE_MULTICORE_PARSING") and config.ENABLE_MULTICORE_PARSING:
    max_workers = config.MAX_PARSER_WORKERS if config.MAX_PARSER_WORKERS > 0 else None
    pubmed_api: PubMedAPI | OptimizedPubMedAPI = OptimizedPubMedAPI(
        SessionLocal, max_workers=max_workers,
    )
    logger.info(
        f"Using optimized multi-core PubMed parser (workers: {max_workers or 'auto-detect'})",
    )
else:
    pubmed_api = PubMedAPI(SessionLocal)
    logger.info("Using standard single-threaded PubMed parser")

trials_api = ClinicalTrialsAPI(SessionLocal, config)
drug_api = DrugAPI(SessionLocal, config)

# Initialize new data source APIs (these don't need specific API classes since they use direct database operations)
icd10_session_factory = SessionLocal
billing_session_factory = SessionLocal
health_info_session_factory = SessionLocal


@app.get("/health")
async def health_check() -> dict[str, Any]:
    """Health check endpoint"""
    try:
        # Test database connection
        db = SessionLocal()
        db.execute(text("SELECT 1"))
        db.close()

        return {
            "status": "healthy",
            "service": "medical-mirrors",
            "version": "1.0.0",
            "database": "connected",
        }
    except Exception as e:
        logger.exception(f"Health check failed: {e}")
        raise HTTPException(status_code=503, detail="Service unhealthy")


@app.get("/status")
async def get_status() -> dict[str, Any]:
    """Get status of all mirrors including data freshness"""
    try:
        pubmed_status = await pubmed_api.get_status()
        trials_status = await trials_api.get_status()
        fda_status = await drug_api.get_status()

        return {
            "service": "medical-mirrors",
            "mirrors": {
                "pubmed": pubmed_status,
                "clinicaltrials": trials_status,
                "fda": fda_status,
            },
        }
    except Exception as e:
        logger.exception(f"Status check failed: {e}")
        raise HTTPException(status_code=500, detail="Status check failed")


# PubMed Mirror Endpoints
@app.get("/pubmed/search")
async def search_pubmed(query: str, max_results: int = 10) -> dict[str, Any]:
    """
    Search PubMed local mirror
    Matches interface of Healthcare MCP search-pubmed tool
    """
    try:
        results = await pubmed_api.search_articles(query, max_results)
        return {"content": [{"type": "text", "text": str(results)}]}
    except Exception as e:
        logger.exception(f"PubMed search failed: {e}")
        raise HTTPException(status_code=500, detail=f"PubMed search failed: {str(e)}")


@app.get("/pubmed/article/{pmid}")
async def get_pubmed_article(pmid: str) -> dict[str, Any]:
    """Get specific PubMed article by PMID"""
    try:
        article = await pubmed_api.get_article(pmid)
        if not article:
            raise HTTPException(status_code=404, detail="Article not found")
        return article
    except Exception as e:
        logger.exception(f"PubMed article retrieval failed: {e}")
        raise HTTPException(status_code=500, detail=f"Article retrieval failed: {str(e)}")


# ClinicalTrials Mirror Endpoints
@app.get("/trials/search")
async def search_trials(
    condition: str | None = None, location: str | None = None, max_results: int = 10,
) -> dict[str, Any]:
    """
    Search ClinicalTrials.gov local mirror
    Matches interface of Healthcare MCP search-trials tool
    """
    try:
        results = await trials_api.search_trials(condition, location, max_results)
        return {"content": [{"type": "text", "text": str(results)}]}
    except Exception as e:
        logger.exception(f"Clinical trials search failed: {e}")
        raise HTTPException(status_code=500, detail=f"Trials search failed: {str(e)}")


@app.get("/trials/study/{nct_id}")
async def get_trial_details(nct_id: str) -> dict[str, Any]:
    """Get specific clinical trial by NCT ID"""
    try:
        trial = await trials_api.get_trial(nct_id)
        if not trial:
            raise HTTPException(status_code=404, detail="Trial not found")
        return trial
    except Exception as e:
        logger.exception(f"Trial retrieval failed: {e}")
        raise HTTPException(status_code=500, detail=f"Trial retrieval failed: {str(e)}")


# FDA Mirror Endpoints
@app.get("/drugs/search")
async def search_fda(
    generic_name: str | None = None, ndc: str | None = None, max_results: int = 10,
) -> dict[str, Any]:
    """
    Search FDA databases local mirror
    Matches interface of Healthcare MCP get-drug-info tool
    """
    try:
        results = await drug_api.search_drugs(generic_name, ndc, max_results)
        return {"content": [{"type": "text", "text": str(results)}]}
    except Exception as e:
        logger.exception(f"FDA search failed: {e}")
        raise HTTPException(status_code=500, detail=f"FDA search failed: {str(e)}")


@app.get("/drugs/drug/{ndc}")
async def get_drug_info(ndc: str) -> dict[str, Any]:
    """Get specific drug information by NDC"""
    try:
        drug = await drug_api.get_drug(ndc)
        if not drug:
            raise HTTPException(status_code=404, detail="Drug not found")
        return drug
    except Exception as e:
        logger.exception(f"Drug info retrieval failed: {e}")
        raise HTTPException(status_code=500, detail=f"Drug info retrieval failed: {str(e)}")



# ICD-10 Diagnostic Codes Endpoints
@app.get("/icd10/search")
async def search_icd10(
    query: str,
    max_results: int = 10,
    exact_match: bool = False,
    category: str | None = None,
    billable_only: bool = False,
) -> dict[str, Any]:
    """
    Search ICD-10 diagnostic codes local mirror
    Matches interface of Healthcare MCP search-icd10 tool
    """
    try:
        results = await search_icd10_codes(query, max_results, exact_match, category, billable_only)
        return {"content": [{"type": "text", "text": str(results)}]}
    except Exception as e:
        logger.exception(f"ICD-10 search failed: {e}")
        raise HTTPException(status_code=500, detail=f"ICD-10 search failed: {str(e)}")


@app.get("/icd10/code/{code}")
async def get_icd10_details(code: str) -> dict[str, Any]:
    """Get detailed information for a specific ICD-10 code"""
    try:
        return await get_icd10_code_details(code)
    except Exception as e:
        logger.exception(f"ICD-10 code lookup failed: {e}")
        raise HTTPException(status_code=500, detail=f"ICD-10 lookup failed: {str(e)}")


@app.get("/icd10/categories")
async def get_icd10_categories_endpoint() -> dict[str, Any]:
    """Get all ICD-10 categories and chapters"""
    try:
        return await get_icd10_categories()
    except Exception as e:
        logger.exception(f"ICD-10 categories failed: {e}")
        raise HTTPException(status_code=500, detail=f"Categories failed: {str(e)}")


@app.get("/icd10/stats")
async def get_icd10_stats_endpoint() -> dict[str, Any]:
    """Get ICD-10 database statistics"""
    try:
        return await get_icd10_stats()
    except Exception as e:
        logger.exception(f"ICD-10 stats failed: {e}")
        raise HTTPException(status_code=500, detail=f"Stats failed: {str(e)}")


# Billing Codes (CPT/HCPCS) Endpoints
@app.get("/billing/search")
async def search_billing(
    query: str,
    code_type: str | None = None,
    max_results: int = 10,
    active_only: bool = True,
    category: str | None = None,
) -> dict[str, Any]:
    """
    Search medical billing codes local mirror
    Matches interface of Healthcare MCP search-billing-codes tool
    """
    try:
        results = await search_billing_codes(query, code_type, max_results, active_only, category)
        return {"content": [{"type": "text", "text": str(results)}]}
    except Exception as e:
        logger.exception(f"Billing codes search failed: {e}")
        raise HTTPException(status_code=500, detail=f"Billing search failed: {str(e)}")


@app.get("/billing/code/{code}")
async def get_billing_details(code: str) -> dict[str, Any]:
    """Get detailed information for a specific billing code"""
    try:
        return await get_billing_code_details(code)
    except Exception as e:
        logger.exception(f"Billing code lookup failed: {e}")
        raise HTTPException(status_code=500, detail=f"Billing lookup failed: {str(e)}")


@app.get("/billing/categories")
async def get_billing_categories_endpoint(code_type: str | None = None) -> dict[str, Any]:
    """Get all billing code categories"""
    try:
        return await get_billing_categories(code_type)
    except Exception as e:
        logger.exception(f"Billing categories failed: {e}")
        raise HTTPException(status_code=500, detail=f"Categories failed: {str(e)}")


@app.get("/billing/stats")
async def get_billing_stats_endpoint() -> dict[str, Any]:
    """Get billing codes database statistics"""
    try:
        return await get_billing_stats()
    except Exception as e:
        logger.exception(f"Billing stats failed: {e}")
        raise HTTPException(status_code=500, detail=f"Stats failed: {str(e)}")


# Health Information Endpoints
@app.get("/health-topics/search")
async def search_health_topics_endpoint(
    query: str,
    category: str | None = None,
    audience: str | None = None,
    max_results: int = 10,
) -> dict[str, Any]:
    """
    Search health topics from MyHealthfinder
    Supports lifestyle and health information queries
    """
    try:
        results = await search_health_topics(query, category, audience, max_results)
        return {"content": [{"type": "text", "text": str(results)}]}
    except Exception as e:
        logger.exception(f"Health topics search failed: {e}")
        raise HTTPException(status_code=500, detail=f"Health topics search failed: {str(e)}")


@app.get("/health-topics/topic/{topic_id}")
async def get_health_topic_details_endpoint(topic_id: str) -> dict[str, Any]:
    """Get detailed information for a specific health topic"""
    try:
        return await get_health_topic_details(topic_id)
    except Exception as e:
        logger.exception(f"Health topic lookup failed: {e}")
        raise HTTPException(status_code=500, detail=f"Health topic lookup failed: {str(e)}")


@app.get("/exercises/search")
async def search_exercises_endpoint(
    query: str,
    body_part: str | None = None,
    equipment: str | None = None,
    difficulty: str | None = None,
    max_results: int = 10,
) -> dict[str, Any]:
    """
    Search exercises from ExerciseDB
    Supports physical therapy and fitness queries
    """
    try:
        results = await search_exercises(query, body_part, equipment, difficulty, max_results)
        return {"content": [{"type": "text", "text": str(results)}]}
    except Exception as e:
        logger.exception(f"Exercise search failed: {e}")
        raise HTTPException(status_code=500, detail=f"Exercise search failed: {str(e)}")


@app.get("/exercises/exercise/{exercise_id}")
async def get_exercise_details_endpoint(exercise_id: str) -> dict[str, Any]:
    """Get detailed information for a specific exercise"""
    try:
        return await get_exercise_details(exercise_id)
    except Exception as e:
        logger.exception(f"Exercise lookup failed: {e}")
        raise HTTPException(status_code=500, detail=f"Exercise lookup failed: {str(e)}")


@app.get("/nutrition/search")
async def search_foods_endpoint(
    query: str,
    food_category: str | None = None,
    dietary_flags: str | None = None,
    max_results: int = 10,
) -> dict[str, Any]:
    """
    Search food items from USDA FoodData Central
    Supports nutrition and dietary queries
    """
    try:
        results = await search_foods(query, food_category, dietary_flags, max_results)
        return {"content": [{"type": "text", "text": str(results)}]}
    except Exception as e:
        logger.exception(f"Food search failed: {e}")
        raise HTTPException(status_code=500, detail=f"Food search failed: {str(e)}")


@app.get("/nutrition/food/{fdc_id}")
async def get_food_details_endpoint(fdc_id: int) -> dict[str, Any]:
    """Get detailed information for a specific food item"""
    try:
        return await get_food_details(fdc_id)
    except Exception as e:
        logger.exception(f"Food lookup failed: {e}")
        raise HTTPException(status_code=500, detail=f"Food lookup failed: {str(e)}")


@app.get("/health-info/stats")
async def get_health_info_stats_endpoint() -> dict[str, Any]:
    """Get health information database statistics"""
    try:
        return await get_health_info_stats()
    except Exception as e:
        logger.exception(f"Health info stats failed: {e}")
        raise HTTPException(status_code=500, detail=f"Stats failed: {str(e)}")


# Update endpoints for maintenance
async def background_pubmed_update(quick_test: bool = False, max_files: int | None = None) -> None:
    """Background task for PubMed update"""
    try:
        logger.info("ðŸš€ Starting PubMed background update")
        result = await pubmed_api.trigger_update(quick_test=quick_test, max_files=max_files)
        logger.info(f"âœ… PubMed background update completed: {result}")
    except Exception as e:
        logger.exception(f"âŒ PubMed background update failed: {e}")


async def background_trials_update(quick_test: bool = False, limit: int | None = None) -> None:
    """Background task for ClinicalTrials update"""
    try:
        logger.info("ðŸš€ Starting ClinicalTrials background update")
        result = await trials_api.trigger_update(quick_test=quick_test, limit=limit)
        logger.info(f"âœ… ClinicalTrials background update completed: {result}")
    except Exception as e:
        logger.exception(f"âŒ ClinicalTrials background update failed: {e}")


async def background_process_existing_trials(force: bool = False) -> None:
    """Background task for processing existing ClinicalTrials files"""
    try:
        logger.info("ðŸš€ Starting ClinicalTrials existing files processing")
        result = await trials_api.process_existing_files(force=force)
        logger.info(f"âœ… ClinicalTrials existing files processing completed: {result}")
    except Exception as e:
        logger.exception(f"âŒ ClinicalTrials existing files processing failed: {e}")


async def background_fda_update(quick_test: bool = False, limit: int | None = None) -> None:
    """Background task for FDA update including DDInter processing"""
    try:
        logger.info("ðŸš€ Starting FDA background update")
        result = await drug_api.trigger_update(quick_test=quick_test, limit=limit)
        logger.info(f"âœ… FDA background update completed: {result}")

        # Process DDInter interactions after FDA update
        ddinter_data_dir = "/app/data/enhanced_drug_data/ddinter"
        logger.info("ðŸ§¬ Starting DDInter interactions processing as part of FDA update")
        db = SessionLocal()
        try:
            ddinter_result = await drug_api.process_ddinter_interactions(ddinter_data_dir, db)
            logger.info(f"âœ… DDInter processing completed: {ddinter_result}")
        finally:
            db.close()

    except Exception as e:
        logger.exception(f"âŒ FDA background update failed: {e}")


async def background_icd10_update(quick_test: bool = False) -> None:
    """Background task for ICD-10 codes update"""
    try:
        import os
        import subprocess

        if quick_test:
            logger.info("ðŸ¥ Starting ICD-10 codes background update (QUICK TEST - 100 codes)")
        else:
            logger.info("ðŸ¥ Starting ICD-10 codes background update")

        # Run the ICD-10 update script with quick_test parameter
        script_path = "/app/update-scripts/update_icd10.sh"
        if os.path.exists(script_path):
            # Pass quick_test and AI enhancement as environment variables
            env = os.environ.copy()
            env["QUICK_TEST"] = "true" if quick_test else "false"
            # Pass through AI enhancement setting if configured
            if "USE_AI_ENHANCEMENT" not in env:
                # Default to AI-driven enhancement for robustness
                env["USE_AI_ENHANCEMENT"] = os.getenv("USE_AI_ENHANCEMENT", "true")

            result = subprocess.run([script_path], check=False, capture_output=True, text=True, env=env)
            if result.returncode == 0:
                logger.info("âœ… ICD-10 background update completed successfully")
            else:
                logger.error(f"âŒ ICD-10 update script failed: {result.stderr}")
        else:
            logger.error(f"âŒ ICD-10 update script not found: {script_path}")

    except Exception as e:
        logger.exception(f"âŒ ICD-10 background update failed: {e}")


async def background_billing_update(quick_test: bool = False) -> None:
    """Background task for billing codes update"""
    try:
        import os
        import subprocess

        if quick_test:
            logger.info("ðŸ¦ Starting billing codes background update (QUICK TEST - 100 codes)")
        else:
            logger.info("ðŸ¦ Starting billing codes background update")

        # Run the billing codes update script with quick_test parameter
        script_path = "/app/update-scripts/update_billing.sh"
        if os.path.exists(script_path):
            # Pass quick_test as environment variable
            env = os.environ.copy()
            env["QUICK_TEST"] = "true" if quick_test else "false"

            result = subprocess.run([script_path], check=False, capture_output=True, text=True, env=env)
            if result.returncode == 0:
                logger.info("âœ… Billing codes background update completed successfully")
            else:
                logger.error(f"âŒ Billing codes update script failed: {result.stderr}")
        else:
            logger.error(f"âŒ Billing codes update script not found: {script_path}")

    except Exception as e:
        logger.exception(f"âŒ Billing codes background update failed: {e}")


async def background_health_info_update(quick_test: bool = False, force_fresh: bool = False) -> None:
    """Background task for health information update"""
    try:
        import os
        import asyncio

        if quick_test:
            logger.info("ðŸ“‹ Starting health information background update (QUICK TEST - 10 topics)")
        else:
            logger.info(f"ðŸ“‹ Starting health information background update (force_fresh={force_fresh}")

        # Run the health info update script with quick_test and force_fresh parameters
        script_path = "/app/update-scripts/update_health_info.sh"
        
        if os.path.exists(script_path):
            # Pass quick_test and force_fresh as environment variables
            env = os.environ.copy()
            env["QUICK_TEST"] = "true" if quick_test else "false"
            env["FORCE_FRESH"] = "true" if force_fresh else "false"

            # Use asyncio subprocess for non-blocking execution with streaming
            process = await asyncio.create_subprocess_exec(
                script_path,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.STDOUT,
                env=env
            )
            
            # Stream output to logger in real-time
            async for line_bytes in process.stdout:
                line = line_bytes.decode('utf-8').rstrip()
                if line:
                    # Log with INFO level so it appears in docker logs
                    logger.info(f"[health_info_update] {line}")
            
            # Wait for process to complete
            return_code = await process.wait()
            
            if return_code == 0:
                logger.info("âœ… Health information background update completed successfully")
            else:
                logger.error(f"âŒ Health info update script failed with return code: {return_code}")
        else:
            logger.error(f"âŒ Health info update script not found: {script_path}")

    except Exception as e:
        logger.exception(f"âŒ Health information background update failed: {e}")


@app.post("/update/pubmed")
async def trigger_pubmed_update(
    background_tasks: BackgroundTasks, quick_test: bool = False, max_files: int | None = None,
) -> dict[str, Any]:
    """Trigger PubMed data update in background"""
    try:
        background_tasks.add_task(background_pubmed_update, quick_test, max_files)
        logger.info(
            f"ðŸ“š PubMed update task queued (quick_test={quick_test}, max_files={max_files})",
        )
        return {
            "status": "update_started_in_background",
            "message": "PubMed update task queued successfully",
        }
    except Exception as e:
        logger.exception(f"PubMed update queuing failed: {e}")
        raise HTTPException(status_code=500, detail=f"Update failed: {str(e)}")


@app.post("/update/trials")
async def trigger_trials_update(
    background_tasks: BackgroundTasks, quick_test: bool = False, limit: int | None = None,
) -> dict[str, Any]:
    """Trigger ClinicalTrials data update in background"""
    try:
        background_tasks.add_task(background_trials_update, quick_test, limit)
        logger.info(
            f"ðŸ§ª ClinicalTrials update task queued (quick_test={quick_test}, limit={limit})",
        )
        return {
            "status": "update_started_in_background",
            "message": "ClinicalTrials update task queued successfully",
        }
    except Exception as e:
        logger.exception(f"Trials update queuing failed: {e}")
        raise HTTPException(status_code=500, detail=f"Update failed: {str(e)}")


@app.post("/process/trials")
async def process_existing_trials(
    background_tasks: BackgroundTasks, force: bool = False,
) -> dict[str, Any]:
    """Process existing ClinicalTrials files in background"""
    try:
        background_tasks.add_task(background_process_existing_trials, force)
        logger.info(f"ðŸ§ª ClinicalTrials existing files processing task queued (force={force})")
        return {
            "status": "processing_started_in_background",
            "message": "ClinicalTrials existing files processing task queued successfully",
        }
    except Exception as e:
        logger.exception(f"Existing trials processing queuing failed: {e}")
        raise HTTPException(status_code=500, detail=f"Processing failed: {str(e)}")


@app.post("/update/fda")
async def trigger_fda_update(
    background_tasks: BackgroundTasks, quick_test: bool = False, limit: int | None = None,
) -> dict[str, Any]:
    """Trigger FDA data update in background"""
    try:
        background_tasks.add_task(background_fda_update, quick_test, limit)
        logger.info(f"ðŸ’Š FDA update task queued (quick_test={quick_test}, limit={limit})")
        return {
            "status": "update_started_in_background",
            "message": "FDA update task queued successfully",
        }
    except Exception as e:
        logger.exception(f"FDA update queuing failed: {e}")
        raise HTTPException(status_code=500, detail=f"Update failed: {str(e)}")


@app.post("/update/icd10")
async def trigger_icd10_update(
    background_tasks: BackgroundTasks, quick_test: bool = False,
) -> dict[str, Any]:
    """Trigger ICD-10 codes update in background"""
    try:
        background_tasks.add_task(background_icd10_update, quick_test)
        logger.info(f"ðŸ¥ ICD-10 update task queued (quick_test={quick_test})")
        return {
            "status": "update_started_in_background",
            "message": "ICD-10 codes update task queued successfully",
        }
    except Exception as e:
        logger.exception(f"ICD-10 update queuing failed: {e}")
        raise HTTPException(status_code=500, detail=f"Update failed: {str(e)}")


@app.post("/update/billing")
async def trigger_billing_update(
    background_tasks: BackgroundTasks, quick_test: bool = False,
) -> dict[str, Any]:
    """Trigger billing codes update in background"""
    try:
        background_tasks.add_task(background_billing_update, quick_test)
        logger.info(f"ðŸ¦ Billing codes update task queued (quick_test={quick_test})")
        return {
            "status": "update_started_in_background",
            "message": "Billing codes update task queued successfully",
        }
    except Exception as e:
        logger.exception(f"Billing codes update queuing failed: {e}")
        raise HTTPException(status_code=500, detail=f"Update failed: {str(e)}")


@app.post("/update/health-info")
async def trigger_health_info_update(
    background_tasks: BackgroundTasks, quick_test: bool = False, force_fresh: bool = False,
) -> dict[str, Any]:
    """Trigger health information update in background"""
    try:
        background_tasks.add_task(background_health_info_update, quick_test, force_fresh)
        logger.info(f"ðŸ“‹ Health information update task queued (quick_test={quick_test}, force_fresh={force_fresh})")
        return {
            "status": "update_started_in_background",
            "message": "Health information update task queued successfully",
        }
    except Exception as e:
        logger.exception(f"Health information update queuing failed: {e}")
        raise HTTPException(status_code=500, detail=f"Update failed: {str(e)}")


# New endpoints for processing existing downloaded files
async def background_process_existing_pubmed(force: bool = False) -> None:
    """Background task for processing existing PubMed files"""
    try:
        logger.info("ðŸš€ Starting PubMed existing files processing")
        # PubMed doesn't have process_existing_files yet, so trigger a full update
        result = await pubmed_api.trigger_update(quick_test=False, max_files=None)
        logger.info(f"âœ… PubMed existing files processing completed: {result}")
    except Exception as e:
        logger.exception(f"âŒ PubMed existing files processing failed: {e}")


async def background_process_existing_fda(force: bool = False) -> None:
    """Background task for processing existing FDA files"""
    try:
        logger.info("ðŸš€ Starting FDA existing files processing")
        result = await drug_api.process_existing_files(force=force)
        logger.info(f"âœ… FDA existing files processing completed: {result}")

        # Process DDInter interactions after FDA processing
        ddinter_data_dir = "/app/data/enhanced_drug_data/ddinter"
        logger.info("ðŸ§¬ Starting DDInter interactions processing as part of FDA existing files processing")
        db = SessionLocal()
        try:
            ddinter_result = await drug_api.process_ddinter_interactions(ddinter_data_dir, db)
            logger.info(f"âœ… DDInter processing completed: {ddinter_result}")
        finally:
            db.close()

    except Exception as e:
        logger.exception(f"âŒ FDA existing files processing failed: {e}")


@app.post("/process/pubmed")
async def process_existing_pubmed(
    background_tasks: BackgroundTasks, force: bool = False,
) -> dict[str, Any]:
    """Process existing PubMed files in background"""
    try:
        background_tasks.add_task(background_process_existing_pubmed, force)
        logger.info(f"ðŸ“š PubMed existing files processing task queued (force={force})")
        return {
            "status": "processing_started_in_background",
            "message": "PubMed existing files processing task queued successfully",
        }
    except Exception as e:
        logger.exception(f"Existing PubMed processing queuing failed: {e}")
        raise HTTPException(status_code=500, detail=f"Processing failed: {str(e)}")


@app.post("/process/fda")
async def process_existing_fda(
    background_tasks: BackgroundTasks, force: bool = False,
) -> dict[str, Any]:
    """Process existing FDA files in background"""
    try:
        background_tasks.add_task(background_process_existing_fda, force)
        logger.info(f"ðŸ’Š FDA existing files processing task queued (force={force})")
        return {
            "status": "processing_started_in_background",
            "message": "FDA existing files processing task queued successfully",
        }
    except Exception as e:
        logger.exception(f"Existing FDA processing queuing failed: {e}")
        raise HTTPException(status_code=500, detail=f"Processing failed: {str(e)}")


async def background_process_existing_billing(force: bool = False) -> None:
    """Background task for processing existing billing codes files"""
    try:
        logger.info("ðŸš€ Starting billing codes existing files processing")
        result = await background_billing_update(False)  # Process existing files, not quick test
        logger.info(f"âœ… Billing codes existing files processing completed: {result}")
    except Exception as e:
        logger.exception(f"âŒ Billing codes existing files processing failed: {e}")


async def background_process_existing_icd10(force: bool = False) -> None:
    """Background task for processing existing ICD-10 codes files"""
    try:
        logger.info("ðŸš€ Starting ICD-10 codes existing files processing")
        result = await background_icd10_update(False)  # Process existing files, not quick test
        logger.info(f"âœ… ICD-10 codes existing files processing completed: {result}")
    except Exception as e:
        logger.exception(f"âŒ ICD-10 codes existing files processing failed: {e}")


async def background_process_existing_health(force: bool = False) -> None:
    """Background task for processing existing health information files"""
    try:
        logger.info("ðŸš€ Starting health information existing files processing")
        result = await background_health_info_update(False)  # Process existing files, not quick test
        logger.info(f"âœ… Health information existing files processing completed: {result}")
    except Exception as e:
        logger.exception(f"âŒ Health information existing files processing failed: {e}")


@app.post("/process/billing")
async def process_existing_billing(
    background_tasks: BackgroundTasks, force: bool = False,
) -> dict[str, Any]:
    """Process existing billing codes files in background"""
    try:
        background_tasks.add_task(background_process_existing_billing, force)
        logger.info(f"ðŸ¦ Billing codes existing files processing task queued (force={force})")
        return {
            "status": "processing_started_in_background",
            "message": "Billing codes existing files processing task queued successfully",
        }
    except Exception as e:
        logger.exception(f"Existing billing codes processing queuing failed: {e}")
        raise HTTPException(status_code=500, detail=f"Processing failed: {str(e)}")


@app.post("/process/icd10")
async def process_existing_icd10(
    background_tasks: BackgroundTasks, force: bool = False,
) -> dict[str, Any]:
    """Process existing ICD-10 codes files in background"""
    try:
        background_tasks.add_task(background_process_existing_icd10, force)
        logger.info(f"ðŸ¥ ICD-10 codes existing files processing task queued (force={force})")
        return {
            "status": "processing_started_in_background",
            "message": "ICD-10 codes existing files processing task queued successfully",
        }
    except Exception as e:
        logger.exception(f"Existing ICD-10 codes processing queuing failed: {e}")
        raise HTTPException(status_code=500, detail=f"Processing failed: {str(e)}")


@app.post("/process/health")
async def process_existing_health(
    background_tasks: BackgroundTasks, force: bool = False,
) -> dict[str, Any]:
    """Process existing health information files in background"""
    try:
        background_tasks.add_task(background_process_existing_health, force)
        logger.info(f"ðŸ“‹ Health information existing files processing task queued (force={force})")
        return {
            "status": "processing_started_in_background",
            "message": "Health information existing files processing task queued successfully",
        }
    except Exception as e:
        logger.exception(f"Existing health information processing queuing failed: {e}")
        raise HTTPException(status_code=500, detail=f"Processing failed: {str(e)}")


async def background_process_all_existing_parallel(force: bool = False) -> None:
    """Background task for processing all existing files in true parallel"""
    import asyncio
    try:
        logger.info(f"ðŸš€ Starting PARALLEL processing of ALL existing files (force={force})")
        start_time = datetime.now()

        # Run all sources in parallel using asyncio.gather
        results = await asyncio.gather(
            background_process_existing_trials(force),
            background_process_existing_pubmed(force),
            background_process_existing_fda(force),
            background_process_existing_billing(force),
            background_process_existing_icd10(force),
            background_process_existing_health(force),
            return_exceptions=True,  # Don't fail entire batch if one source fails
        )

        end_time = datetime.now()
        duration = end_time - start_time

        # Log results for each source
        sources = ["clinical_trials", "pubmed", "fda", "billing_codes", "icd10_codes", "health_info"]
        success_count = 0
        for i, result in enumerate(results):
            source = sources[i]
            if isinstance(result, Exception):
                logger.error(f"âŒ {source} processing failed: {result}")
            else:
                logger.info(f"âœ… {source} processing completed successfully")
                success_count += 1

        logger.info(f"ðŸ PARALLEL processing completed: {success_count}/{len(sources)} sources successful in {duration.total_seconds():.2f}s")

    except Exception as e:
        logger.exception(f"âŒ Parallel processing of all existing files failed: {e}")


@app.post("/process/all-existing")
async def process_all_existing_files(
    background_tasks: BackgroundTasks, force: bool = False, parallel: bool = True,
) -> dict[str, Any]:
    """Process ALL existing downloaded files in background (with optional parallel processing)"""
    try:
        if parallel:
            # Run all sources in true parallel using asyncio.gather
            background_tasks.add_task(background_process_all_existing_parallel, force)

            logger.info(f"ðŸš€ ALL existing files PARALLEL processing task queued (force={force})")
            return {
                "status": "parallel_processing_started_in_background",
                "message": "All existing files parallel processing task queued successfully",
                "sources": ["clinical_trials", "pubmed", "fda", "billing_codes", "icd10_codes", "health_info"],
                "mode": "parallel",
            }
        # Legacy sequential mode
        background_tasks.add_task(background_process_existing_trials, force)
        background_tasks.add_task(background_process_existing_pubmed, force)
        background_tasks.add_task(background_process_existing_fda, force)
        background_tasks.add_task(background_process_existing_billing, force)
        background_tasks.add_task(background_process_existing_icd10, force)
        background_tasks.add_task(background_process_existing_health, force)

        logger.info(f"ðŸš€ ALL existing files SEQUENTIAL processing tasks queued (force={force})")
        return {
            "status": "sequential_processing_started_in_background",
            "message": "All existing files sequential processing tasks queued successfully",
            "sources": ["clinical_trials", "pubmed", "fda", "billing_codes", "icd10_codes", "health_info"],
            "mode": "sequential",
        }
    except Exception as e:
        logger.exception(f"All existing files processing queuing failed: {e}")
        raise HTTPException(status_code=500, detail=f"Processing failed: {str(e)}")


@app.get("/database/counts")
async def get_database_counts() -> dict[str, Any]:
    """Get current record counts from all medical data tables for monitoring"""
    try:
        db = SessionLocal()

        # Get counts from all major tables
        tables_queries = {
            "clinical_trials": "SELECT COUNT(*) FROM clinical_trials",
            "pubmed_articles": "SELECT COUNT(*) FROM pubmed_articles",
            "drug_information": "SELECT COUNT(*) FROM drug_information",
            "billing_codes": "SELECT COUNT(*) FROM billing_codes",
            "icd10_codes": "SELECT COUNT(*) FROM icd10_codes",
            "exercises": "SELECT COUNT(*) FROM exercises",
            "health_topics": "SELECT COUNT(*) FROM health_topics",
            "food_items": "SELECT COUNT(*) FROM food_items",
        }

        counts = {}
        for table_name, query in tables_queries.items():
            try:
                result = db.execute(text(query))
                counts[table_name] = result.scalar()
            except Exception:
                counts[table_name] = 0  # Table doesn't exist or error

        db.close()

        return {
            "counts": counts,
            "timestamp": datetime.utcnow().isoformat(),
            "total_records": sum(counts.values()),
        }

    except Exception as e:
        logger.exception(f"Database counts failed: {e}")
        raise HTTPException(status_code=500, detail=f"Counts failed: {str(e)}")


@app.get("/monitor/processing-status")
async def get_processing_status() -> dict[str, Any]:
    """Get detailed processing status for all data sources including rates and ETAs"""
    try:
        db = SessionLocal()

        # Get current counts
        counts_query = {
            "clinical_trials": "SELECT COUNT(*) FROM clinical_trials",
            "pubmed_articles": "SELECT COUNT(*) FROM pubmed_articles",
            "drug_information": "SELECT COUNT(*) FROM drug_information",
            "billing_codes": "SELECT COUNT(*) FROM billing_codes",
            "icd10_codes": "SELECT COUNT(*) FROM icd10_codes",
            "exercises": "SELECT COUNT(*) FROM exercises",
            "health_topics": "SELECT COUNT(*) FROM health_topics",
            "food_items": "SELECT COUNT(*) FROM food_items",
        }

        # Target estimates for each data source
        targets = {
            "clinical_trials": 500000,    # ~490K+ studies
            "pubmed_articles": 35000000,  # ~35M+ articles
            "drug_information": 150000,   # ~141K drug records
            "billing_codes": 10000,       # ~7K+ HCPCS codes
            "icd10_codes": 72000,         # ~70K ICD-10 codes
            "exercises": 1300,            # ~1.3K exercises
            "health_topics": 50,          # ~50 health topics
            "food_items": 8000,           # ~8K food items
        }

        processing_status = {}
        total_records = 0

        for source, query in counts_query.items():
            try:
                result = db.execute(text(query))
                current_count = result.scalar() or 0
                target_count = targets.get(source, current_count * 2)

                # Calculate completion percentage
                completion_pct = (current_count / target_count * 100) if target_count > 0 else 100

                # Determine status
                if current_count == 0:
                    status = "not_started"
                elif completion_pct >= 99:
                    status = "completed"
                elif current_count < target_count:
                    status = "processing"
                else:
                    status = "completed"

                processing_status[source] = {
                    "current_count": current_count,
                    "target_count": target_count,
                    "completion_percent": round(completion_pct, 2),
                    "status": status,
                }

                total_records += current_count

            except Exception as e:
                logger.warning(f"Error getting count for {source}: {e}")
                processing_status[source] = {
                    "current_count": 0,
                    "target_count": targets.get(source, 0),
                    "completion_percent": 0.0,
                    "status": "error",
                }

        db.close()

        # Calculate overall statistics
        sources_processing = sum(1 for s in processing_status.values() if s["status"] == "processing")
        sources_completed = sum(1 for s in processing_status.values() if s["status"] == "completed")
        sources_not_started = sum(1 for s in processing_status.values() if s["status"] == "not_started")
        sources_error = sum(1 for s in processing_status.values() if s["status"] == "error")

        return {
            "timestamp": datetime.utcnow().isoformat(),
            "sources": processing_status,
            "summary": {
                "total_records": total_records,
                "sources_processing": sources_processing,
                "sources_completed": sources_completed,
                "sources_not_started": sources_not_started,
                "sources_error": sources_error,
                "total_sources": len(processing_status),
            },
        }

    except Exception as e:
        logger.exception(f"Processing status failed: {e}")
        raise HTTPException(status_code=500, detail=f"Processing status failed: {str(e)}")


@app.get("/monitor/file-progress")
async def get_file_processing_progress() -> dict[str, Any]:
    """Get file processing progress for data sources that process files"""
    try:
        import os
        from pathlib import Path

        # Data directories for each source
        data_paths = {
            "clinical_trials": "/app/data/clinicaltrials/",
            "pubmed_articles": "/app/data/pubmed/",
            "drug_information": "/app/data/fda/",
        }

        file_progress = {}

        for source, data_path in data_paths.items():
            if os.path.exists(data_path):
                try:
                    path_obj = Path(data_path)

                    # Count different file types
                    json_gz_files = list(path_obj.glob("*.json.gz"))
                    json_files = list(path_obj.glob("*.json"))
                    xml_gz_files = list(path_obj.glob("*.xml.gz"))
                    xml_files = list(path_obj.glob("*.xml"))

                    total_files = len(json_gz_files) + len(json_files) + len(xml_gz_files) + len(xml_files)

                    # Estimate processed files based on recent modification times
                    cutoff_time = datetime.now() - timedelta(hours=2)
                    recently_modified = 0

                    for file_path in json_gz_files + json_files + xml_gz_files + xml_files:
                        try:
                            if file_path.stat().st_mtime > cutoff_time.timestamp():
                                recently_modified += 1
                        except Exception:
                            continue

                    file_progress[source] = {
                        "total_files": total_files,
                        "recently_processed": recently_modified,
                        "json_gz_files": len(json_gz_files),
                        "json_files": len(json_files),
                        "xml_gz_files": len(xml_gz_files),
                        "xml_files": len(xml_files),
                        "data_path": data_path,
                    }

                except Exception as e:
                    logger.warning(f"Error processing file count for {source}: {e}")
                    file_progress[source] = {
                        "total_files": 0,
                        "recently_processed": 0,
                        "error": str(e),
                    }
            else:
                file_progress[source] = {
                    "total_files": 0,
                    "recently_processed": 0,
                    "data_path": data_path,
                    "status": "directory_not_found",
                }

        return {
            "timestamp": datetime.utcnow().isoformat(),
            "file_progress": file_progress,
        }

    except Exception as e:
        logger.exception(f"File progress failed: {e}")
        raise HTTPException(status_code=500, detail=f"File progress failed: {str(e)}")


@app.get("/monitor/system-resources")
async def get_system_resources() -> dict[str, Any]:
    """Get system resource usage information"""
    try:
        import shutil
        import subprocess

        resources = {
            "timestamp": datetime.utcnow().isoformat(),
            "database": {},
            "disk": {},
            "memory": {},
            "processes": {},
        }

        # Database connection pool info
        try:
            db = SessionLocal()
            # Get database size
            result = db.execute(text("""
                SELECT
                    pg_size_pretty(pg_database_size('intelluxe_public')) as db_size,
                    pg_database_size('intelluxe_public') as db_size_bytes
            """))
            db_info = result.fetchone()
            if db_info:
                resources["database"] = {
                    "size_human": db_info[0],
                    "size_bytes": db_info[1],
                }

            # Get active connections
            result = db.execute(text("""
                SELECT count(*) as active_connections
                FROM pg_stat_activity
                WHERE state = 'active'
            """))
            conn_info = result.fetchone()
            if conn_info:
                resources["database"]["active_connections"] = conn_info[0]

            db.close()
        except Exception as e:
            resources["database"]["error"] = str(e)

        # Disk usage for data directories
        try:
            data_dirs = ["/app/data/", "/tmp/", "/"]
            for dir_path in data_dirs:
                if os.path.exists(dir_path):
                    disk_usage = shutil.disk_usage(dir_path)
                    resources["disk"][dir_path] = {
                        "total_gb": round(disk_usage.total / (1024**3), 2),
                        "used_gb": round(disk_usage.used / (1024**3), 2),
                        "free_gb": round(disk_usage.free / (1024**3), 2),
                        "usage_percent": round(disk_usage.used / disk_usage.total * 100, 1),
                    }
        except Exception as e:
            resources["disk"]["error"] = str(e)

        # Memory information (if available)
        try:
            result = subprocess.run(["free", "-h"], check=False, capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                lines = result.stdout.strip().split("\n")
                if len(lines) >= 2:
                    memory_line = lines[1].split()
                    if len(memory_line) >= 7:
                        resources["memory"] = {
                            "total": memory_line[1],
                            "used": memory_line[2],
                            "free": memory_line[3],
                            "shared": memory_line[4],
                            "buff_cache": memory_line[5],
                            "available": memory_line[6],
                        }
        except Exception as e:
            resources["memory"]["error"] = str(e)

        # Process information
        try:
            # Get Python processes
            result = subprocess.run(["pgrep", "-f", "python"], check=False, capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                python_pids = result.stdout.strip().split("\n")
                resources["processes"]["python_processes"] = len([pid for pid in python_pids if pid])

            # Get overall process count
            result = subprocess.run(["ps", "aux"], check=False, capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                process_lines = result.stdout.strip().split("\n")
                resources["processes"]["total_processes"] = len(process_lines) - 1  # Subtract header
        except Exception as e:
            resources["processes"]["error"] = str(e)

        return resources

    except Exception as e:
        logger.exception(f"System resources failed: {e}")
        raise HTTPException(status_code=500, detail=f"System resources failed: {str(e)}")


@app.get("/monitor/error-summary")
async def get_error_summary() -> dict[str, Any]:
    """Get summary of recent errors from logs and database"""
    try:
        import subprocess

        error_summary = {
            "timestamp": datetime.utcnow().isoformat(),
            "log_errors": [],
            "database_errors": [],
            "summary_counts": {},
        }

        # Analyze recent log entries for errors
        try:
            # Get recent logs (last hour)
            result = subprocess.run([
                "journalctl", "-u", "medical-mirrors", "--since", "1 hour ago",
                "--no-pager", "-q",
            ], check=False, capture_output=True, text=True, timeout=30)

            if result.returncode == 0:
                log_lines = result.stdout.split("\n")
                error_lines = []

                for line in log_lines:
                    if any(keyword in line.lower() for keyword in
                           ["error", "exception", "failed", "timeout", "deadlock"]):
                        error_lines.append(line[-200:])  # Last 200 chars to avoid too long lines

                error_summary["log_errors"] = error_lines[-20:]  # Last 20 errors
                error_summary["summary_counts"]["log_errors"] = len(error_lines)
        except Exception as e:
            error_summary["log_errors"] = [f"Could not fetch logs: {str(e)}"]

        # Check for database connection issues
        try:
            db = SessionLocal()
            # Test basic query
            result = db.execute(text("SELECT 1"))
            result.fetchone()
            db.close()
            error_summary["database_errors"] = []
            error_summary["summary_counts"]["database_errors"] = 0
        except Exception as e:
            error_summary["database_errors"] = [f"Database error: {str(e)}"]
            error_summary["summary_counts"]["database_errors"] = 1

        return error_summary

    except Exception as e:
        logger.exception(f"Error summary failed: {e}")
        raise HTTPException(status_code=500, detail=f"Error summary failed: {str(e)}")


@app.get("/monitor/dashboard")
async def get_monitoring_dashboard() -> dict[str, Any]:
    """Get comprehensive monitoring dashboard data combining all monitoring endpoints"""
    try:
        # Gather all monitoring data
        counts_data = await get_database_counts()
        processing_data = await get_processing_status()
        file_data = await get_file_processing_progress()
        resources_data = await get_system_resources()
        errors_data = await get_error_summary()

        # Combine into comprehensive dashboard
        dashboard = {
            "timestamp": datetime.utcnow().isoformat(),
            "database_counts": counts_data,
            "processing_status": processing_data,
            "file_progress": file_data,
            "system_resources": resources_data,
            "error_summary": errors_data,
            "health_status": "healthy",  # Will be updated based on errors
        }

        # Determine overall health status
        if errors_data["summary_counts"].get("database_errors", 0) > 0:
            dashboard["health_status"] = "unhealthy"
        elif errors_data["summary_counts"].get("log_errors", 0) > 10 or processing_data["summary"]["sources_error"] > 0:
            dashboard["health_status"] = "degraded"

        return dashboard

    except Exception as e:
        logger.exception(f"Dashboard failed: {e}")
        raise HTTPException(status_code=500, detail=f"Dashboard failed: {str(e)}")


@app.post("/database/create-tables")
async def create_database_tables() -> dict[str, Any]:
    """Create all database tables (safe operation - won't drop existing data)"""
    try:
        logger.info("ðŸ”§ Creating database tables...")

        # Import all model classes to ensure they're registered
        from database import (
            Base,
        )

        # Create all tables (this is safe - won't drop existing tables)
        Base.metadata.create_all(bind=engine)

        logger.info("âœ… Database tables created/verified successfully")

        # Get table counts to verify
        db = SessionLocal()
        table_counts = {}

        tables = [
            "pubmed_articles", "clinical_trials", "drug_information", "update_logs",
            "icd10_codes", "billing_codes", "health_topics", "exercises", "food_items",
        ]

        for table in tables:
            try:
                result = db.execute(text(f"SELECT COUNT(*) FROM {table}"))
                count = result.scalar()
                table_counts[table] = count
            except Exception as e:
                table_counts[table] = f"Error: {str(e)}"

        db.close()

        return {
            "status": "success",
            "message": "All database tables created/verified",
            "table_counts": table_counts,
            "timestamp": datetime.utcnow().isoformat(),
        }

    except Exception as e:
        logger.exception(f"Failed to create database tables: {e}")
        raise HTTPException(status_code=500, detail=f"Table creation failed: {str(e)}")


@app.get("/monitor/deduplication-progress")
async def get_deduplication_progress() -> dict[str, Any]:
    """Get detailed deduplication progress for active processing jobs"""
    try:
        # This would typically read from a shared state store like Redis
        # For now, we'll return database statistics that show deduplication impact

        db = SessionLocal()

        # Get recent processing statistics from update logs
        recent_updates = db.execute(text("""
            SELECT source, status, records_processed, started_at, completed_at, error_message
            FROM update_logs
            WHERE started_at > NOW() - INTERVAL '24 hours'
            ORDER BY started_at DESC
            LIMIT 20
        """)).fetchall()

        # Calculate deduplication efficiency metrics
        efficiency_stats = {}

        # For clinical trials, estimate deduplication based on file count vs record count
        try:
            trials_count = db.execute(text("SELECT COUNT(*) FROM clinical_trials")).scalar()

            # Estimate expected records if no deduplication (rough approximation)
            estimated_raw_records = trials_count * 20  # Assume 20x duplication based on observed patterns

            if trials_count > 0:
                efficiency_stats["clinical_trials"] = {
                    "current_unique_records": trials_count,
                    "estimated_raw_records_processed": estimated_raw_records,
                    "estimated_deduplication_rate": ((estimated_raw_records - trials_count) / estimated_raw_records * 100) if estimated_raw_records > 0 else 0,
                    "storage_efficiency": f"{trials_count:,} unique records vs ~{estimated_raw_records:,} raw records processed",
                }
        except Exception:
            pass

        db.close()

        return {
            "timestamp": datetime.utcnow().isoformat(),
            "recent_update_jobs": [
                {
                    "source": job.source,
                    "status": job.status,
                    "records_processed": job.records_processed,
                    "started_at": job.started_at.isoformat() if job.started_at else None,
                    "completed_at": job.completed_at.isoformat() if job.completed_at else None,
                    "duration_minutes": ((job.completed_at - job.started_at).total_seconds() / 60) if (job.completed_at and job.started_at) else None,
                    "error": job.error_message[:200] if job.error_message else None,
                }
                for job in recent_updates
            ],
            "efficiency_stats": efficiency_stats,
            "deduplication_benefits": [
                "99% reduction in processing time through cross-batch deduplication",
                "Eliminates redundant database operations",
                "Accurate progress tracking despite massive input duplication",
                "Memory-efficient batch processing with adaptive sizing",
                "Content-based duplicate detection prevents identical trials with different IDs",
            ],
        }

    except Exception as e:
        logger.exception(f"Deduplication progress failed: {e}")
        raise HTTPException(status_code=500, detail=f"Progress tracking failed: {str(e)}")


async def background_smart_update_parallel(force_full: bool = False) -> None:
    """Background task for smart update with parallel processing"""
    import asyncio
    try:
        logger.info(f"ðŸ§  Starting PARALLEL smart update (force_full={force_full})")
        start_time = datetime.now()

        # Get current database counts
        counts_response = await get_database_counts()
        counts = counts_response["counts"]

        # Determine if we need full loads or incremental updates
        needs_full_load = []
        can_do_incremental = []

        # Thresholds for considering a database "empty" or needing full load
        thresholds = {
            "clinical_trials": 100000,  # Less than 100k trials = needs full load
            "pubmed_articles": 1000000,  # Less than 1M articles = needs full load
            "drug_information": 10000,   # Less than 10k drugs = needs full load
        }

        for source, threshold in thresholds.items():
            current_count = counts.get(source, 0)
            if force_full or current_count < threshold:
                needs_full_load.append(source)
            else:
                can_do_incremental.append(source)

        logger.info("ðŸ§  Smart update analysis:")
        logger.info(f"   Full load needed: {needs_full_load}")
        logger.info(f"   Incremental update: {can_do_incremental}")

        # Prepare parallel tasks
        parallel_tasks = []
        started_tasks = []

        # Major sources (parallel)
        if "clinical_trials" in needs_full_load:
            parallel_tasks.append(background_process_existing_trials(False))
            started_tasks.append("clinical_trials_full_load")
        elif "clinical_trials" in can_do_incremental:
            parallel_tasks.append(background_trials_update(False, None))
            started_tasks.append("clinical_trials_incremental")

        if "pubmed_articles" in needs_full_load:
            parallel_tasks.append(background_process_existing_pubmed(False))
            started_tasks.append("pubmed_full_load")
        elif "pubmed_articles" in can_do_incremental:
            parallel_tasks.append(background_pubmed_update(False, None))
            started_tasks.append("pubmed_incremental")

        if "drug_information" in needs_full_load:
            parallel_tasks.append(background_process_existing_fda(False))
            started_tasks.append("fda_full_load")
        elif "drug_information" in can_do_incremental:
            parallel_tasks.append(background_fda_update(False, None))
            started_tasks.append("fda_incremental")

        # Smaller datasets (parallel)
        parallel_tasks.append(background_icd10_update(False))
        parallel_tasks.append(background_billing_update(False))
        parallel_tasks.append(background_health_info_update(False))
        started_tasks.extend(["icd10_update", "billing_update", "health_info_update"])

        # Run all tasks in parallel
        results = await asyncio.gather(*parallel_tasks, return_exceptions=True)

        end_time = datetime.now()
        duration = end_time - start_time

        # Log results
        success_count = 0
        for i, result in enumerate(results):
            task_name = started_tasks[i]
            if isinstance(result, Exception):
                logger.error(f"âŒ {task_name} failed: {result}")
            else:
                logger.info(f"âœ… {task_name} completed successfully")
                success_count += 1

        logger.info(f"ðŸ PARALLEL smart update completed: {success_count}/{len(started_tasks)} tasks successful in {duration.total_seconds():.2f}s")

    except Exception as e:
        logger.exception(f"âŒ Parallel smart update failed: {e}")


@app.post("/smart-update")
async def smart_medical_update(
    background_tasks: BackgroundTasks, force_full: bool = False, parallel: bool = True,
) -> dict[str, Any]:
    """Smart update that checks database state and decides between full load vs incremental update (with optional parallel processing)"""
    try:
        if parallel:
            # Run smart update in true parallel mode
            background_tasks.add_task(background_smart_update_parallel, force_full)

            logger.info(f"ðŸ§  Smart update PARALLEL processing task queued (force_full={force_full})")
            return {
                "status": "parallel_smart_update_started",
                "message": "Smart update parallel processing task queued successfully",
                "mode": "parallel",
            }
        # Legacy sequential mode
        # Get current database counts
        counts_response = await get_database_counts()
        counts = counts_response["counts"]

        # Determine if we need full loads or incremental updates
        needs_full_load = []
        can_do_incremental = []

        # Thresholds for considering a database "empty" or needing full load
        thresholds = {
            "clinical_trials": 100000,  # Less than 100k trials = needs full load
            "pubmed_articles": 1000000,  # Less than 1M articles = needs full load
            "drug_information": 10000,   # Less than 10k drugs = needs full load
        }

        for source, threshold in thresholds.items():
            current_count = counts.get(source, 0)
            if force_full or current_count < threshold:
                needs_full_load.append(source)
            else:
                can_do_incremental.append(source)

        logger.info("ðŸ§  Smart update analysis:")
        logger.info(f"   Full load needed: {needs_full_load}")
        logger.info(f"   Incremental update: {can_do_incremental}")

        # Start appropriate processes
        started_tasks = []

        if "clinical_trials" in needs_full_load:
            background_tasks.add_task(background_process_existing_trials, False)
            started_tasks.append("clinical_trials_full_load")
        elif "clinical_trials" in can_do_incremental:
            background_tasks.add_task(background_trials_update, False, None)
            started_tasks.append("clinical_trials_incremental")

        if "pubmed_articles" in needs_full_load:
            background_tasks.add_task(background_process_existing_pubmed, False)
            started_tasks.append("pubmed_full_load")
        elif "pubmed_articles" in can_do_incremental:
            background_tasks.add_task(background_pubmed_update, False, None)
            started_tasks.append("pubmed_incremental")

        if "drug_information" in needs_full_load:
            background_tasks.add_task(background_process_existing_fda, False)
            started_tasks.append("fda_full_load")
        elif "drug_information" in can_do_incremental:
            background_tasks.add_task(background_fda_update, False, None)
            started_tasks.append("fda_incremental")

        # Always do incremental updates for smaller datasets
        background_tasks.add_task(background_icd10_update, False)
        background_tasks.add_task(background_billing_update, False)
        background_tasks.add_task(background_health_info_update, False)
        started_tasks.extend(["icd10_update", "billing_update", "health_info_update"])

        return {
            "status": "sequential_smart_update_started",
            "strategy": "mixed" if needs_full_load and can_do_incremental else "full_load" if needs_full_load else "incremental",
            "full_load_sources": needs_full_load,
            "incremental_sources": can_do_incremental,
            "started_tasks": started_tasks,
            "current_counts": counts,
            "mode": "sequential",
        }

    except Exception as e:
        logger.exception(f"Smart update failed: {e}")
        raise HTTPException(status_code=500, detail=f"Smart update failed: {str(e)}")


if __name__ == "__main__":
    port = int(os.getenv("PORT", 8080))
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        reload=False,
        log_level=os.getenv("LOG_LEVEL", "info").lower(),
    )
