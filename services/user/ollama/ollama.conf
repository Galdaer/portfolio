# Ollama Local AI Model Server for Intelluxe AI Healthcare System
# Privacy-first local LLM serving with HIPAA compliance for clinical workflows
image=ollama/ollama:latest
port=11434
description=Local AI model server for Intelluxe AI healthcare workflows with HIPAA compliance
# Ollama API server for clinical AI inference
env=OLLAMA_HOST=0.0.0.0,OLLAMA_KEEP_ALIVE=24h,OLLAMA_MAX_LOADED_MODELS=3,HEALTHCARE_MODE=true
# Shared model storage for efficient healthcare AI deployment
volumes=shared-ollama-models:/root/.ollama,shared-ollama-config:/config:ro,intelluxe-audit-logs:/app/logs
network_mode=intelluxe-net
static_ip=172.20.0.10
restart_policy=unless-stopped
domain_routing=true
healthcheck=ollama list
labels=traefik.enable=true,traefik.http.services.ollama.loadbalancer.server.port=11434,traefik.http.routers.ollama.rule=Host(`ollama.galdaer.duckdns.org`),traefik.http.routers.ollama.entrypoints=websecure,traefik.http.routers.ollama.tls=true,traefik.http.routers.ollama.tls.certresolver=letsencrypt
# Maximum GPU utilization for healthcare AI workloads with audit logging
gpus=all
runtime=nvidia
memory=20g
read_only=false
security_opt=no-new-privileges