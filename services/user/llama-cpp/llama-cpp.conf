# Llama.cpp Server for Intelluxe AI Healthcare System
# Direct hardware control and fine-tuning capabilities for clinical AI workflows
image=ghcr.io/ggerganov/llama.cpp:server
port=8080:8080
description=Advanced LLM server with hardware optimization and fine-tuning support for healthcare AI
# Llama.cpp server with OpenAI API compatibility - using env vars
env=HOST=0.0.0.0,PORT=8080,HEALTHCARE_MODE=true,N_CTX=8192,N_GPU_LAYERS=35,N_THREADS=16,LLAMA_ARG_MODEL=/models/default.gguf,LLAMA_ARG_CTX_SIZE=8192,LLAMA_ARG_PORT=8080
# Shared model storage and configuration
volumes=shared-llama-models:/models,shared-llama-config:/config:ro,intelluxe-audit-logs:/app/logs
network_mode=intelluxe-net
static_ip=172.20.0.19
restart_policy=unless-stopped
domain_routing=true
healthcheck=curl -f http://localhost:8080/health || exit 1
# Maximum GPU utilization with fine-grained control
gpus=all
runtime=nvidia
memory=24g
read_only=false
security_opt=no-new-privileges
# Additional llama.cpp specific optimizations
cpu_limit=16
shm_size=2g
ulimit=memlock=-1:-1
ulimit=stack=67108864:67108864
